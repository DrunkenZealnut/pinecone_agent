#!/usr/bin/env python3
"""
Pinecone Agent Web Interface
Flask-based web UI for Pinecone vector database operations with RAG support.
"""

import os
import re
import json
import unicodedata
from pathlib import Path
from flask import Flask, render_template, request, jsonify, Response, stream_with_context, send_from_directory
from dotenv import load_dotenv
from openai import OpenAI
import sys

# Import calculator modules
sys.path.append(str(Path(__file__).parent))
from calculator.wage_calculator import WageCalculator
from calculator.insurance_calculator import InsuranceCalculator, CompanySize, IndustryType
from msds_client import MsdsApiClient

# Load environment variables
load_dotenv()

app = Flask(__name__)
app.config['SECRET_KEY'] = os.urandom(24)

# Documents folder path for serving images
DOCUMENTS_PATH = Path(__file__).parent / "documents"

# Domain-specific system prompts
DOMAIN_PROMPTS = {
    "laborlaw": """ë‹¹ì‹ ì€ í•œêµ­ ë…¸ë™ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ë²•ë¥  ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

## ê³„ì‚°ê¸° ë„êµ¬ ì‚¬ìš© (ì¤‘ìš”)

**ì„ê¸ˆ ë° ë³´í—˜ë£Œ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° ë°˜ë“œì‹œ ì œê³µëœ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:**

1. **calculate_wage**: ì‹¤ìˆ˜ë ¹ì•¡, 4ëŒ€ë³´í—˜ë£Œ, ì†Œë“ì„¸ ê³„ì‚°
   - ì‚¬ìš© ì‹œê¸°: ì—°ë´‰/ì›”ê¸‰ì—ì„œ ì‹¤ì œ ë°›ëŠ” ê¸‰ì—¬ë¥¼ ê³„ì‚°í•  ë•Œ
   - ì˜ˆì‹œ: "ì—°ë´‰ 5000ë§Œì› ì‹¤ìˆ˜ë ¹ì•¡ì€?", "ì›”ê¸‰ 300ë§Œì› ì„¸ê¸ˆ ì–¼ë§ˆ?"

2. **calculate_insurance**: 4ëŒ€ë³´í—˜ë£Œ ìƒì„¸ ê³„ì‚°
   - ì‚¬ìš© ì‹œê¸°: ì—…ì¢…ë³„/ê·œëª¨ë³„ ë³´í—˜ë£Œë¥¼ ìì„¸íˆ ì•Œê³  ì‹¶ì„ ë•Œ
   - ì˜ˆì‹œ: "ê±´ì„¤ì—… 4ëŒ€ë³´í—˜ë£ŒëŠ”?", "300ë§Œì› êµ­ë¯¼ì—°ê¸ˆ ì–¼ë§ˆ?"

**í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ í›„ì—ëŠ” ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì •ë¦¬í•˜ì—¬ ë³´ê¸° ì¢‹ê²Œ ì œì‹œí•˜ì„¸ìš”.**

## í•„ìˆ˜ ê³„ì‚° ê·œì¹™ (ìˆ˜ë™ ê³„ì‚° ì‹œ)

### ì£¼íœ´ìˆ˜ë‹¹
- ì¡°ê±´: ì£¼ 15ì‹œê°„ ì´ìƒ ê·¼ë¬´ ì‹œ ì˜ë¬´ ì§€ê¸‰
- ê³µì‹: ì£¼íœ´ìˆ˜ë‹¹ = (ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ Ã· 40) Ã— 8 Ã— ì‹œê¸‰
- **ì£¼ê¸‰/ì›”ê¸‰ ê³„ì‚° ì‹œ ë°˜ë“œì‹œ ì£¼íœ´ìˆ˜ë‹¹ì„ í¬í•¨í•˜ì—¬ ë‹µë³€**

### ì›”ê¸‰ ê³„ì‚°
- ì›” í™˜ì‚°: ì£¼ë‹¹ì‹œê°„ Ã— 4.345ì£¼
- ì£¼íœ´ í¬í•¨ ì›”ê¸‰: (ì£¼ë‹¹ ê·¼ë¬´ì‹œê°„ + ì£¼íœ´ì‹œê°„) Ã— 4.345 Ã— ì‹œê¸‰

### ê°€ì‚°ìˆ˜ë‹¹
- ì—°ì¥ê·¼ë¡œ(8ì‹œê°„ ì´ˆê³¼): í†µìƒì„ê¸ˆ Ã— 1.5ë°°
- ì•¼ê°„ê·¼ë¡œ(22ì‹œ~06ì‹œ): í†µìƒì„ê¸ˆ Ã— 1.5ë°°
- íœ´ì¼ê·¼ë¡œ: í†µìƒì„ê¸ˆ Ã— 1.5ë°° (8ì‹œê°„ ì´ˆê³¼ ì‹œ 2.0ë°°)

## ë‹µë³€ ì§€ì¹¨
1. **ì„ê¸ˆ/ë³´í—˜ë£Œ ê³„ì‚° ì‹œ í•¨ìˆ˜ë¥¼ ìš°ì„  ì‚¬ìš©**í•˜ê³ , ê²°ê³¼ë¥¼ í‘œë¡œ ì •ë¦¬
2. ê³„ì‚° ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…
3. ê´€ë ¨ ë²•ì¡°í•­ ì¸ìš© ì‹œ [1], [2] í˜•ì‹ ì‚¬ìš©
4. ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ê³„ì‚° ë‚´ì—­ ì •ë¦¬
5. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”""",
}

# Default system prompt (semiconductor/general domain)
DEFAULT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë°˜ë„ì²´ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ì§€ì¹¨:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. **ì¤‘ìš”**: ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ë°˜ë“œì‹œ ì¸ìš© ë²ˆí˜¸ë¡œ í‘œì‹œí•˜ì„¸ìš”. ì˜ˆ: "CVD ê³µì •ì€ í™”í•™ ê¸°ìƒ ì¦ì°© ë°©ì‹ì…ë‹ˆë‹¤[1]."
3. ì¸ìš© í˜•ì‹: ë¬¸ì¥ ëì— [1], [2] ë“±ì˜ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ì–´ë–¤ ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”
4. ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¢…í•©í•  ë•ŒëŠ” [1][3]ì²˜ëŸ¼ ë³µìˆ˜ ì¸ìš©ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤
5. ê¸°ìˆ  ìš©ì–´ëŠ” í•œê¸€ê³¼ ì˜ë¬¸ì„ ë³‘ê¸°í•˜ì„¸ìš”
6. í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì„¸ìš”
7. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
8. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”
9. **ì¤‘ìš”**: ì´ë¯¸ì§€ëŠ” ì ˆëŒ€ ì§ì ‘ ì‚½ì…í•˜ì§€ ë§ˆì„¸ìš” (![image](...) í˜•ì‹ ì‚¬ìš© ê¸ˆì§€). ê´€ë ¨ ì´ë¯¸ì§€ëŠ” ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."""

# Global instances (lazy initialization)
_agent = None
_openai_client = None

def get_openai_client():
    """Get or create OpenAI client."""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    return _openai_client

def get_agent():
    """Get or create the PineconeAgent instance."""
    global _agent
    if _agent is None:
        from src.agent import PineconeAgent
        _agent = PineconeAgent(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            pinecone_api_key=os.getenv("PINECONE_API_KEY"),
            pinecone_index_name=os.getenv("PINECONE_INDEX_NAME", "document-index"),
            create_index_if_not_exists=False
        )
    return _agent

def get_uploader():
    """Get PineconeUploader for stats."""
    from src.pinecone_uploader import PineconeUploader
    return PineconeUploader(
        api_key=os.getenv("PINECONE_API_KEY"),
        index_name=os.getenv("PINECONE_INDEX_NAME", "document-index"),
        create_if_not_exists=False
    )


# ============================================
# Calculator Functions for GPT Function Calling
# ============================================

def calculate_wage(
    salary_type: str,
    amount: int,
    tax_free_monthly: int = 0,
    dependents: int = 1,
    children_8_to_20: int = 0,
    company_size: str = 'small'
) -> dict:
    """
    ì„ê¸ˆ ê³„ì‚° (ì‹¤ìˆ˜ë ¹ì•¡, 4ëŒ€ë³´í—˜ë£Œ, ì„¸ê¸ˆ)

    Args:
        salary_type: 'ì—°ë´‰' ë˜ëŠ” 'ì›”ê¸‰'
        amount: ê¸‰ì—¬ì•¡ (ì›)
        tax_free_monthly: ì›” ë¹„ê³¼ì„¸ì•¡ (ìµœëŒ€ 200,000ì›)
        dependents: ë¶€ì–‘ê°€ì¡± ìˆ˜ (ë³¸ì¸ í¬í•¨, ê¸°ë³¸ 1ëª…)
        children_8_to_20: 8~20ì„¸ ìë…€ ìˆ˜ (ê¸°ë³¸ 0ëª…)
        company_size: íšŒì‚¬ ê·œëª¨ ('small': 150ì¸ ë¯¸ë§Œ, 'medium': 150~999ì¸, 'large': 1000ì¸ ì´ìƒ)

    Returns:
        dict: ê³„ì‚° ê²°ê³¼ (ì…ë ¥ì •ë³´, ê·¼ë¡œìê³µì œë‚´ì—­, íšŒì‚¬ë¶€ë‹´ë‚´ì—­, ì‹¤ìˆ˜ë ¹ì•¡)
    """
    calc = WageCalculator()

    if salary_type == 'ì—°ë´‰':
        result = calc.calculate_from_annual(
            annual_salary=amount,
            tax_free_monthly=tax_free_monthly,
            dependents=dependents,
            children_8_to_20=children_8_to_20,
            company_size=company_size
        )
    else:  # ì›”ê¸‰
        result = calc.calculate_from_monthly(
            monthly_salary=amount,
            tax_free_monthly=tax_free_monthly,
            dependents=dependents,
            children_8_to_20=children_8_to_20,
            company_size=company_size
        )

    return result


def calculate_insurance(
    monthly_income: int,
    non_taxable: int = 0,
    company_size_code: str = 'UNDER_150',
    industry_code: str = 'OTHERS'
) -> dict:
    """
    4ëŒ€ë³´í—˜ë£Œ ê³„ì‚° (êµ­ë¯¼ì—°ê¸ˆ, ê±´ê°•ë³´í—˜, ì¥ê¸°ìš”ì–‘ë³´í—˜, ê³ ìš©ë³´í—˜, ì‚°ì¬ë³´í—˜)

    Args:
        monthly_income: ì›” ì†Œë“ (ì›)
        non_taxable: ë¹„ê³¼ì„¸ì†Œë“ (ì›)
        company_size_code: íšŒì‚¬ ê·œëª¨ ì½”ë“œ
            - 'UNDER_150': 150ì¸ ë¯¸ë§Œ
            - 'PRIORITY_SUPPORT': 150ì¸ ì´ìƒ ìš°ì„ ì§€ì›ëŒ€ìƒê¸°ì—…
            - 'FROM_150_TO_999': 150~999ì¸
            - 'OVER_1000': 1000ì¸ ì´ìƒ
        industry_code: ì—…ì¢… ì½”ë“œ (ì‚°ì¬ë³´í—˜ ìš”ìœ¨)
            - 'OTHERS': ê¸°íƒ€ì‚¬ì—… (0.9%)
            - 'WHOLESALE_RETAIL': ë„ì†Œë§¤/ìŒì‹/ìˆ™ë°• (0.7%)
            - 'PROFESSIONAL': ì „ë¬¸/ê³¼í•™/ê¸°ìˆ  (0.5%)
            - 'FINANCE_INSURANCE': ê¸ˆìœµ/ë³´í—˜ (0.5%)
            - 'CONSTRUCTION': ê±´ì„¤ì—… (3.5%)
            - ê¸°íƒ€: IndustryType enum ì°¸ê³ 

    Returns:
        dict: ë³´í—˜ë£Œ ìƒì„¸ ë‚´ì—­ (ì…ë ¥ì •ë³´, ê° ë³´í—˜ë³„ ë‚´ì—­, í•©ê³„)
    """
    calc = InsuranceCalculator()

    # Convert string codes to enums
    size_map = {
        'UNDER_150': CompanySize.UNDER_150,
        'PRIORITY_SUPPORT': CompanySize.PRIORITY_SUPPORT,
        'FROM_150_TO_999': CompanySize.FROM_150_TO_999,
        'OVER_1000': CompanySize.OVER_1000
    }
    company_size = size_map.get(company_size_code, CompanySize.UNDER_150)

    # Find industry type
    industry = IndustryType.OTHERS
    for ind in IndustryType:
        if ind.name == industry_code:
            industry = ind
            break

    result = calc.calculate_all(
        monthly_income=monthly_income,
        non_taxable=non_taxable,
        company_size=company_size,
        industry=industry
    )

    return result


# GPT Function definitions for Function Calling
CALCULATOR_FUNCTIONS = [
    {
        "type": "function",
        "function": {
            "name": "calculate_wage",
            "description": "í•œêµ­ ë…¸ë™ë²•ì— ë”°ë¥¸ ì„ê¸ˆ ê³„ì‚° (ì‹¤ìˆ˜ë ¹ì•¡, 4ëŒ€ë³´í—˜ë£Œ, ì†Œë“ì„¸, ì§€ë°©ì†Œë“ì„¸). ì—°ë´‰ì´ë‚˜ ì›”ê¸‰ì„ ì…ë ¥í•˜ë©´ ì‹¤ì œ ë°›ê²Œ ë˜ëŠ” ê¸‰ì—¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "salary_type": {
                        "type": "string",
                        "enum": ["ì—°ë´‰", "ì›”ê¸‰"],
                        "description": "ê¸‰ì—¬ ìœ í˜•"
                    },
                    "amount": {
                        "type": "integer",
                        "description": "ê¸‰ì—¬ì•¡ (ì› ë‹¨ìœ„)"
                    },
                    "tax_free_monthly": {
                        "type": "integer",
                        "description": "ì›” ë¹„ê³¼ì„¸ì•¡ (ì‹ëŒ€ ë“±, ìµœëŒ€ 200,000ì›)",
                        "default": 0
                    },
                    "dependents": {
                        "type": "integer",
                        "description": "ë¶€ì–‘ê°€ì¡± ìˆ˜ (ë³¸ì¸ í¬í•¨)",
                        "default": 1
                    },
                    "children_8_to_20": {
                        "type": "integer",
                        "description": "8~20ì„¸ ìë…€ ìˆ˜ (ìë…€ì„¸ì•¡ê³µì œ ëŒ€ìƒ)",
                        "default": 0
                    },
                    "company_size": {
                        "type": "string",
                        "enum": ["small", "medium", "large"],
                        "description": "íšŒì‚¬ ê·œëª¨: small(150ì¸ ë¯¸ë§Œ), medium(150~999ì¸), large(1000ì¸ ì´ìƒ)",
                        "default": "small"
                    }
                },
                "required": ["salary_type", "amount"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculate_insurance",
            "description": "í•œêµ­ 4ëŒ€ë³´í—˜ë£Œ ìƒì„¸ ê³„ì‚° (êµ­ë¯¼ì—°ê¸ˆ, ê±´ê°•ë³´í—˜, ì¥ê¸°ìš”ì–‘ë³´í—˜, ê³ ìš©ë³´í—˜, ì‚°ì¬ë³´í—˜). ê·¼ë¡œìì™€ ì‚¬ì—…ì£¼ê°€ ê°ê° ë¶€ë‹´í•˜ëŠ” ë³´í—˜ë£Œë¥¼ ì—…ì¢…ê³¼ íšŒì‚¬ ê·œëª¨ì— ë”°ë¼ ê³„ì‚°í•©ë‹ˆë‹¤.",
            "parameters": {
                "type": "object",
                "properties": {
                    "monthly_income": {
                        "type": "integer",
                        "description": "ì›” ì†Œë“ (ì› ë‹¨ìœ„)"
                    },
                    "non_taxable": {
                        "type": "integer",
                        "description": "ë¹„ê³¼ì„¸ì†Œë“ (ì› ë‹¨ìœ„)",
                        "default": 0
                    },
                    "company_size_code": {
                        "type": "string",
                        "enum": ["UNDER_150", "PRIORITY_SUPPORT", "FROM_150_TO_999", "OVER_1000"],
                        "description": "íšŒì‚¬ ê·œëª¨: UNDER_150(150ì¸ ë¯¸ë§Œ), PRIORITY_SUPPORT(ìš°ì„ ì§€ì›ëŒ€ìƒ), FROM_150_TO_999(150~999ì¸), OVER_1000(1000ì¸ ì´ìƒ)",
                        "default": "UNDER_150"
                    },
                    "industry_code": {
                        "type": "string",
                        "description": "ì‚°ì¬ë³´í—˜ ì—…ì¢… ì½”ë“œ (ì˜ˆ: OTHERS-ê¸°íƒ€ì‚¬ì—…, WHOLESALE_RETAIL-ë„ì†Œë§¤, PROFESSIONAL-ì „ë¬¸ì„œë¹„ìŠ¤, FINANCE_INSURANCE-ê¸ˆìœµ, CONSTRUCTION-ê±´ì„¤)",
                        "default": "OTHERS"
                    }
                },
                "required": ["monthly_income"]
            }
        }
    }
]


def parse_mentions(query):
    """Parse @mentions from query and return (clean_query, filters).

    Supports:
    - @íŒŒì¼ëª….md - specific file
    - @í´ë”ëª…/ - folder path (ends with /)
    - @í‚¤ì›Œë“œ - partial match on source_file
    """
    mentions = re.findall(r'@([^\s@]+)', query)
    clean_query = re.sub(r'@[^\s@]+', '', query).strip()

    filters = []
    for mention in mentions:
        if mention.endswith('/'):
            # Folder filter
            filters.append({'type': 'folder', 'value': mention.rstrip('/')})
        elif '.' in mention:
            # File filter (has extension)
            filters.append({'type': 'file', 'value': mention})
        else:
            # Keyword filter
            filters.append({'type': 'keyword', 'value': mention})

    return clean_query, filters


def build_source_filter(filters):
    """Build Pinecone filter from parsed mentions.

    Note: Pinecone doesn't support substring matching directly,
    so we'll filter results post-query.
    """
    # For now, return None and do post-filtering
    # Pinecone filter would require exact match or $in operator
    return None


# Domain configurations
DOMAIN_CONFIG = {
    'semiconductor': {
        'title': 'ë°˜ë„ì²´ AI ê²€ìƒ‰',
        'icon': 'ğŸ”¬',
        'namespace': '',  # default namespace
        'color': '#00d4ff',
        'color_rgb': '0, 212, 255',
        'gradient_from': '#00d4ff',
        'gradient_to': '#0099cc',
        'description': 'ë°˜ë„ì²´ ê¸°ìˆ  ë¬¸ì„œ ê¸°ë°˜ AI ì§ˆì˜ì‘ë‹µ',
        'sample_questions': [
            'CVDì™€ PVD ê³µì •ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?',
            'ë°˜ë„ì²´ íŒ¨í‚¤ì§• ê³µì •ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”',
            'ì›¨ì´í¼ ì œì¡° ê³¼ì •ì˜ ì£¼ìš” ë‹¨ê³„ëŠ”?'
        ],
        'features': ['ê³µì • ê¸°ìˆ ', 'ì•„í‚¤í…ì²˜', 'ì œì¡° ê³µì •', 'í’ˆì§ˆ ê´€ë¦¬']
    },
    'laborlaw': {
        'title': 'ë…¸ë™ë²• AI ìƒë‹´',
        'icon': 'âš–ï¸',
        'namespace': 'laborlaw',
        'color': '#ff9800',
        'color_rgb': '255, 152, 0',
        'gradient_from': '#ff9800',
        'gradient_to': '#f57c00',
        'description': 'ë…¸ë™ë²• ë° ì„ê¸ˆÂ·ë³´í—˜ë£Œ ê³„ì‚° ì „ë¬¸ AI',
        'sample_questions': [
            'ì—°ë´‰ 5000ë§Œì› ì‹¤ìˆ˜ë ¹ì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?',
            'ì£¼íœ´ìˆ˜ë‹¹ ê³„ì‚° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”',
            'ë¶€ë‹¹í•´ê³  íŒë‹¨ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?',
            '4ëŒ€ë³´í—˜ë£ŒëŠ” ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?'
        ],
        'features': ['ì„ê¸ˆ ê³„ì‚°', '4ëŒ€ë³´í—˜', 'ê·¼ë¡œê¸°ì¤€ë²•', 'íŒë¡€ ê²€ìƒ‰', 'ë²•ë¥  ìƒë‹´']
    },
    'msds': {
        'title': 'MSDS í™”í•™ë¬¼ì§ˆ ì •ë³´',
        'icon': 'ğŸ§ª',
        'namespace': 'msds',
        'color': '#4caf50',
        'color_rgb': '76, 175, 80',
        'gradient_from': '#4caf50',
        'gradient_to': '#388e3c',
        'description': 'ì‚°ì—…ì•ˆì „ë³´ê±´ê³µë‹¨ ë¬¼ì§ˆì•ˆì „ë³´ê±´ìë£Œ ê²€ìƒ‰',
        'sample_questions': [
            'ë²¤ì  ì˜ ì•ˆì „ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”',
            'ì—íƒ„ì˜¬ì˜ ìœ í•´ì„±Â·ìœ„í—˜ì„±ì€?',
            'í†¨ë£¨ì—”ì˜ ì‘ê¸‰ì¡°ì¹˜ìš”ë ¹ì„ ì°¾ì•„ì¤˜',
            'CAS ë²ˆí˜¸ë¡œ í™”í•™ë¬¼ì§ˆ ê²€ìƒ‰'
        ],
        'features': ['í™”í•™ë¬¼ì§ˆ ê²€ìƒ‰', 'MSDS ì •ë³´', 'ì•ˆì „ ë°ì´í„°', 'CAS ë²ˆí˜¸ ì¡°íšŒ']
    }
}


@app.route('/')
def home():
    """Home page with domain selection."""
    return render_template('home.html')


@app.route('/semiconductor')
def semiconductor():
    """Semiconductor domain page."""
    config = DOMAIN_CONFIG['semiconductor']
    return render_template('domain.html', domain='semiconductor', config=config)


@app.route('/laborlaw')
def laborlaw():
    """Labor law domain page."""
    config = DOMAIN_CONFIG['laborlaw']
    return render_template('domain.html', domain='laborlaw', config=config)


@app.route('/msds')
def msds():
    """MSDS chemical information page."""
    config = DOMAIN_CONFIG['msds']
    return render_template('msds.html', domain='msds', config=config)


@app.route('/documents/<path:filepath>')
def serve_document(filepath):
    """Serve document images and files."""
    return send_from_directory(DOCUMENTS_PATH, filepath)


def find_related_images(source_file: str) -> list:
    """Find related images from the same document folder."""
    import logging

    images = []
    try:
        # Extract the folder path from source_file
        # e.g., "ncs/ë°˜ë„ì²´ê°œë°œ/LM1903060102_23v5_ë°˜ë„ì²´_ì•„í‚¤í…ì²˜_ì„¤ê³„/..." -> folder path
        parts = source_file.split('/')
        if len(parts) >= 3:
            folder_path = DOCUMENTS_PATH / parts[0] / parts[1] / parts[2]
            if folder_path.exists():
                # Find all image files in the folder
                for ext in ['*.jpeg', '*.jpg', '*.png']:
                    for img_file in folder_path.glob(ext):
                        rel_path = img_file.relative_to(DOCUMENTS_PATH)
                        images.append({
                            'path': f'/documents/{rel_path}',
                            'name': img_file.name
                        })
    except Exception as e:
        logging.warning(f"Error finding images: {e}")
    return images[:10]  # Limit to 10 images


@app.route('/api/stats')
def api_stats():
    """Get index statistics."""
    try:
        uploader = get_uploader()
        stats = uploader.get_stats()

        # Format namespaces for frontend
        namespaces = []
        if stats.get('namespaces'):
            for ns_name, ns_info in stats['namespaces'].items():
                namespaces.append({
                    'name': ns_name if ns_name else '(ê¸°ë³¸)',
                    'vector_count': ns_info.vector_count
                })

        return jsonify({
            'success': True,
            'data': {
                'index_name': os.getenv("PINECONE_INDEX_NAME", "document-index"),
                'dimension': stats.get('dimension', 0),
                'total_vectors': stats.get('total_vector_count', 0),
                'namespaces': namespaces
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/search', methods=['POST'])
def api_search():
    """Search for similar content."""
    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        top_k = int(data.get('top_k', 5))
        namespace = data.get('namespace', '')
        file_type = data.get('file_type', '')

        if not query:
            return jsonify({'success': False, 'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        agent = get_agent()

        # Build filter
        filter_dict = None
        if file_type:
            filter_dict = {"file_type": file_type}

        results = agent.search(
            query=query,
            top_k=top_k,
            namespace=namespace,
            filter=filter_dict
        )

        # Format results for frontend
        formatted_results = []
        for r in results:
            metadata = r.get('metadata', {})
            formatted_results.append({
                'score': round(r.get('score', 0), 4),
                'source_file': metadata.get('source_file', 'N/A'),
                'file_type': metadata.get('file_type', 'N/A'),
                'content': metadata.get('content', '')[:500],
                'filename': metadata.get('filename', ''),
                'relative_path': metadata.get('relative_path', '')
            })

        return jsonify({
            'success': True,
            'data': {
                'query': query,
                'count': len(formatted_results),
                'results': formatted_results
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/ask', methods=['POST'])
def api_ask():
    """RAG endpoint - search and generate comprehensive answer."""
    import logging

    try:
        data = request.get_json()
        query = data.get('query', '').strip()
        namespace = data.get('namespace', '')
        top_k = int(data.get('top_k', 10))  # More documents for better context

        logging.info(f"[API/ask] Query: {query[:50]}..., Namespace: {namespace}, TopK: {top_k}")

        if not query:
            return jsonify({'success': False, 'error': 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        # Parse @mentions for source filtering
        clean_query, mention_filters = parse_mentions(query)

        # Build search query: include filter keywords to improve relevance
        if mention_filters:
            filter_keywords = ' '.join([f['value'].replace('_', ' ') for f in mention_filters])
            if clean_query and len(clean_query) >= 3:
                search_query = f"{filter_keywords} {clean_query}"
            else:
                search_query = filter_keywords
        else:
            search_query = clean_query if clean_query else query

        agent = get_agent()
        client = get_openai_client()

        # Step 1: Search for relevant documents (fetch more if filtering)
        # When filtering, we need to fetch significantly more results
        search_top_k = top_k * 5 if mention_filters else top_k
        results = agent.search(
            query=search_query,
            top_k=search_top_k,
            namespace=namespace
        )

        # Step 1.5: Apply mention filters (post-query filtering)
        # Note: Use Unicode NFC normalization to handle Korean character encoding differences
        if mention_filters and results:
            filtered_results = []
            for r in results:
                source_file = unicodedata.normalize('NFC', r.get('metadata', {}).get('source_file', ''))
                filename = unicodedata.normalize('NFC', r.get('metadata', {}).get('filename', ''))

                match = False
                for f in mention_filters:
                    filter_value = unicodedata.normalize('NFC', f['value'].lower())
                    if f['type'] == 'file':
                        if filter_value in filename.lower():
                            match = True
                            break
                    elif f['type'] == 'folder':
                        if filter_value in source_file.lower():
                            match = True
                            break
                    elif f['type'] == 'keyword':
                        if filter_value in source_file.lower():
                            match = True
                            break

                if match:
                    filtered_results.append(r)

            results = filtered_results[:top_k]

        if not results:
            return jsonify({
                'success': True,
                'data': {
                    'answer': 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                    'sources': []
                }
            })

        # Step 2: Build context from search results
        context_parts = []
        sources = []

        for i, r in enumerate(results):
            metadata = r.get('metadata', {})
            content = metadata.get('content', '')
            source_file = metadata.get('source_file', 'Unknown')
            file_type = metadata.get('file_type', 'unknown')
            score = r.get('score', 0)

            if content:
                context_parts.append(f"[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {source_file})\n{content}")

                # Build image URL for image files
                image_url = None
                if file_type == 'image' or source_file.lower().endswith(('.jpeg', '.jpg', '.png', '.gif')):
                    image_url = f'/documents/{source_file}'

                sources.append({
                    'source_file': source_file,
                    'file_type': file_type,
                    'score': round(score, 4),
                    'content_preview': content[:200] + '...' if len(content) > 200 else content,
                    'image_url': image_url
                })

        context = "\n\n---\n\n".join(context_parts)

        # Step 3: Generate comprehensive answer using GPT
        # Select domain-specific prompt based on namespace
        base_prompt = DOMAIN_PROMPTS.get(namespace, DEFAULT_SYSTEM_PROMPT)

        # Common visual representation guidelines (appended to all prompts)
        visual_guidelines = """

ì‹œê°ì  í‘œí˜„ ì§€ì¹¨:
10. **ë¹„êµ ì •ë³´**ëŠ” ë°˜ë“œì‹œ í‘œ(table)ë¡œ ì •ë¦¬í•˜ì„¸ìš”
   ì˜ˆì‹œ:
   | êµ¬ë¶„ | í•­ëª©1 | í•­ëª©2 |
   |------|-------|-------|
   | íŠ¹ì§• | ì„¤ëª…1 | ì„¤ëª…2 |

11. **í”„ë¡œì„¸ìŠ¤/ë‹¨ê³„**ëŠ” ìˆœì„œ ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”
12. **ìˆ˜ì¹˜ ë°ì´í„°**ê°€ ìˆìœ¼ë©´ ì •ë¦¬í•´ì„œ ì œì‹œí•˜ì„¸ìš” (ì°¨íŠ¸ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆë„ë¡)
13. **ë¶„ë¥˜/ì¢…ë¥˜**ëŠ” í‘œë‚˜ ëª©ë¡ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”

ë°ì´í„° ì‹œê°í™” ìš”ì²­ì‹œ:
- ìˆ˜ì¹˜ ë¹„êµê°€ í•„ìš”í•œ ê²½ìš°, ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ ì œê³µí•˜ì„¸ìš”:
<!--CHART_DATA
{
  "type": "bar|pie|line",
  "title": "ì°¨íŠ¸ ì œëª©",
  "labels": ["ë¼ë²¨1", "ë¼ë²¨2"],
  "data": [ìˆ˜ì¹˜1, ìˆ˜ì¹˜2],
  "unit": "ë‹¨ìœ„"
}
CHART_DATA-->"""

        system_prompt = base_prompt + visual_guidelines

        user_prompt = f"""## ì§ˆë¬¸
{query}

## ì°¸ê³  ë¬¸ì„œ
{context}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
**ë°˜ë“œì‹œ ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ [1], [2] ë“±ì˜ ì¸ìš© ë²ˆí˜¸ë¡œ í‘œì‹œí•˜ì„¸ìš”.**"""

        # Enable calculator functions only for laborlaw namespace
        tools = CALCULATOR_FUNCTIONS if namespace == 'laborlaw' else None

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # Initial GPT call (with function calling if tools available)
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            tools=tools,
            tool_choice="auto" if tools else None,
            temperature=0.3,
            max_tokens=2000
        )

        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        # Handle function calls
        calculation_results = []
        if tool_calls:
            messages.append(response_message)

            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)

                logging.info(f"[Function Call] {function_name} with args: {function_args}")

                # Execute the function
                if function_name == "calculate_wage":
                    function_response = calculate_wage(**function_args)
                elif function_name == "calculate_insurance":
                    function_response = calculate_insurance(**function_args)
                else:
                    function_response = {"error": "Unknown function"}

                calculation_results.append({
                    'function': function_name,
                    'args': function_args,
                    'result': function_response
                })

                # Add function response to messages
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": function_name,
                    "content": json.dumps(function_response, ensure_ascii=False, indent=2)
                })

            # Get final response with function results
            second_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.3,
                max_tokens=2000
            )

            answer = second_response.choices[0].message.content
        else:
            answer = response_message.content

        # Collect related images from source documents
        related_images = []
        seen_folders = set()
        for source in sources:
            source_file = source.get('source_file', '')
            # Extract folder path to avoid duplicates
            folder_key = '/'.join(source_file.split('/')[:3])
            if folder_key not in seen_folders:
                seen_folders.add(folder_key)
                images = find_related_images(source_file)
                related_images.extend(images)

        # Limit total images
        related_images = related_images[:12]

        return jsonify({
            'success': True,
            'data': {
                'query': query,
                'answer': answer,
                'sources': sources,
                'source_count': len(sources),
                'images': related_images,
                'calculations': calculation_results if calculation_results else None
            }
        })

    except Exception as e:
        import logging
        logging.exception(f"[API/ask] Error: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/namespaces')
def api_namespaces():
    """Get list of namespaces."""
    try:
        uploader = get_uploader()
        stats = uploader.get_stats()

        namespaces = []
        if stats.get('namespaces'):
            for ns_name in stats['namespaces'].keys():
                namespaces.append(ns_name if ns_name else '')

        return jsonify({
            'success': True,
            'data': namespaces
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/sources')
def api_sources():
    """Get list of available source files and folders for autocomplete."""
    try:
        namespace = request.args.get('namespace', '')
        agent = get_agent()

        # Use generic queries based on namespace to find documents
        # Different queries for different document domains
        generic_queries = ["ë¬¸ì„œ", "ì •ë³´", "ë‚´ìš©", "ë²•ë¥ ", "ê·œì •", "ê¸°ìˆ "]

        all_results = []
        for query in generic_queries[:2]:  # Use top 2 queries to reduce API calls
            results = agent.search(
                query=query,
                top_k=50,
                namespace=namespace
            )
            all_results.extend(results)

        # Deduplicate by source_file
        seen = set()
        results = []
        for r in all_results:
            source_file = r.get('metadata', {}).get('source_file', '')
            if source_file and source_file not in seen:
                seen.add(source_file)
                results.append(r)

        folders = set()
        files = set()

        for r in results:
            metadata = r.get('metadata', {})
            source_file = metadata.get('source_file', '')
            filename = metadata.get('filename', '')

            if filename:
                files.add(filename)

            if source_file:
                # Extract folder paths
                parts = source_file.split('/')
                for i in range(1, len(parts)):
                    folder = '/'.join(parts[:i])
                    if folder:
                        folders.add(folder)

        return jsonify({
            'success': True,
            'data': {
                'folders': sorted(folders),
                'files': sorted(files)
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/delete', methods=['POST'])
def api_delete():
    """Delete vectors."""
    try:
        data = request.get_json()
        namespace = data.get('namespace', '')
        delete_all = data.get('delete_all', False)
        source_file = data.get('source_file', '')

        uploader = get_uploader()

        if delete_all:
            uploader.index.delete(delete_all=True, namespace=namespace)
            return jsonify({
                'success': True,
                'message': f"ë„¤ì„ìŠ¤í˜ì´ìŠ¤ '{namespace or '(ê¸°ë³¸)'}' ì˜ ëª¨ë“  ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            })
        elif source_file:
            success = uploader.delete_by_filter(
                filter={"source_file": source_file},
                namespace=namespace
            )
            if success:
                return jsonify({
                    'success': True,
                    'message': f"'{source_file}'ì˜ ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                })
            else:
                return jsonify({'success': False, 'error': 'ì‚­ì œ ì‹¤íŒ¨'})
        else:
            return jsonify({'success': False, 'error': 'ì‚­ì œí•  ëŒ€ìƒì„ ì§€ì •í•´ì£¼ì„¸ìš”.'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


# MSDS API Client instance
msds_client = MsdsApiClient()


@app.route('/api/msds/search', methods=['POST'])
def msds_search():
    """Search chemicals in KOSHA MSDS database."""
    try:
        data = request.json
        search_word = data.get('search_word', '')
        search_type = int(data.get('search_type', 0))  # 0=êµ­ë¬¸ëª…, 1=CAS, 2=UN, 3=KE, 4=EN
        page_no = int(data.get('page_no', 1))
        num_of_rows = int(data.get('num_of_rows', 10))

        if not search_word:
            return jsonify({'success': False, 'error': 'ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        result = msds_client.search_chemicals(
            search_word=search_word,
            search_type=search_type,
            page_no=page_no,
            num_of_rows=num_of_rows
        )

        return jsonify(result)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/msds/detail', methods=['POST'])
def msds_detail():
    """Get detailed chemical information from KOSHA MSDS."""
    try:
        data = request.json
        chem_id = data.get('chem_id', '')
        section = data.get('section', '')  # Optional: specific section (01-16)

        if not chem_id:
            return jsonify({'success': False, 'error': 'í™”í•™ë¬¼ì§ˆ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'})

        # If section is specified, get that section only
        if section:
            result = msds_client.get_chemical_detail(chem_id, section)
        else:
            # Get all sections
            result = msds_client.get_full_chemical_detail(chem_id)

        return jsonify(result)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/msds/identify', methods=['POST'])
def msds_identify():
    """Identify chemical substance from image using OpenAI Vision API."""
    try:
        data = request.json
        image_data = data.get('image', '')  # Base64 encoded image

        if not image_data:
            return jsonify({'success': False, 'error': 'ì´ë¯¸ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'})

        # Remove data URL prefix if present
        if ',' in image_data:
            image_data = image_data.split(',')[1]

        # Use OpenAI Vision API to identify chemical
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": """ì´ë¯¸ì§€ì—ì„œ í™”í•™ë¬¼ì§ˆ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
1. í™”í•™ë¬¼ì§ˆëª… (êµ­ë¬¸ëª… ë˜ëŠ” ì˜ë¬¸ëª…)
2. CAS ë²ˆí˜¸ (ìˆëŠ” ê²½ìš°)
3. ì œí’ˆëª… ë˜ëŠ” ìƒí’ˆëª… (ìˆëŠ” ê²½ìš°)

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”:
{
    "chemical_name": "í™”í•™ë¬¼ì§ˆëª…",
    "cas_no": "CAS ë²ˆí˜¸ (ì—†ìœ¼ë©´ null)",
    "product_name": "ì œí’ˆëª… (ì—†ìœ¼ë©´ null)"
}

ì´ë¯¸ì§€ì—ì„œ í™”í•™ë¬¼ì§ˆ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´:
{
    "chemical_name": null,
    "cas_no": null,
    "product_name": null,
    "error": "í™”í•™ë¬¼ì§ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
}"""
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=500
        )

        # Parse response
        result_text = response.choices[0].message.content.strip()

        # Extract JSON from response
        import json
        import re

        # Try to find JSON in the response
        json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group())
        else:
            # If no JSON found, try to parse the whole response
            result = json.loads(result_text)

        # If chemical name found, search for it
        if result.get('chemical_name'):
            search_word = result['chemical_name']
            search_type = 0  # Default to Korean name

            # If CAS number is available, use that for search
            if result.get('cas_no'):
                search_word = result['cas_no']
                search_type = 1  # CAS number search

            # Search in MSDS database
            search_result = msds_client.search_chemicals(
                search_word=search_word,
                search_type=search_type,
                page_no=1,
                num_of_rows=10
            )

            return jsonify({
                'success': True,
                'identified': result,
                'search_result': search_result
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'í™”í•™ë¬¼ì§ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'),
                'identified': result
            })

    except json.JSONDecodeError as e:
        return jsonify({
            'success': False,
            'error': f'ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}',
            'raw_response': result_text if 'result_text' in locals() else None
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


if __name__ == '__main__':
    # Check environment variables
    if not os.getenv("OPENAI_API_KEY"):
        print("Error: OPENAI_API_KEY not set")
        exit(1)
    if not os.getenv("PINECONE_API_KEY"):
        print("Error: PINECONE_API_KEY not set")
        exit(1)

    print("ğŸš€ Pinecone Agent Web Interface")
    print("=" * 40)
    print(f"Index: {os.getenv('PINECONE_INDEX_NAME', 'document-index')}")
    print("=" * 40)
    print("\nğŸŒ http://localhost:5001 ì—ì„œ ì ‘ì†í•˜ì„¸ìš”\n")

    app.run(debug=True, host='0.0.0.0', port=5001)
